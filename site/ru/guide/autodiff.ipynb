{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### Авторские права 2020 Авторы TensorFlow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
        },
        "colab_type": "code",
        "id": "tuOe1ymfHZPu"
      },
      "outputs": [

      ],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qFdPvlXBOdUN"
      },
      "source": [
        "# Введение в градиенты и автоматическое дифференцирование"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MfBg1C5NB3X0"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td><a target=\"_blank\" href=\"https://www.tensorflow.org/guide/autodiff\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\"> Посмотреть на TensorFlow.org</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/autodiff.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\"> Запустить в Google Colab</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/guide/autodiff.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\"> Посмотреть источник на GitHub</a></td>\n",
        "  <td><a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/guide/autodiff.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\"> Скачать блокнот</a></td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r6P32iYYV27b"
      },
      "source": [
        "## Автоматическое дифференцирование и градиенты\n",
        "\n",
        "[Автоматическое дифференцирование](https://en.wikipedia.org/wiki/Automatic_differentiation) полезно для реализации алгоритмов машинного обучения, таких как [обратное распространение](https://en.wikipedia.org/wiki/Backpropagation) для обучения нейронных сетей.\n",
        "\n",
        "В этом руководстве мы обсудим способы вычисления градиентов с помощью TensorFlow, особенно в стремительном исполнении."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MUXex9ctTuDB"
      },
      "source": [
        "## Настроить"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "IqR2PQG4ZaZ0"
      },
      "outputs": [

      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xHxb-dlhMIzW"
      },
      "source": [
        "## Вычисление градиентов\n",
        "\n",
        "Для автоматической дифференциации TensorFlow необходимо запомнить, какие операции происходят в каком порядке во время *прямого* прохода. Затем во время *обратного прохода* TensorFlow перебирает этот список операций в обратном порядке для вычисления градиентов."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1CLWJl0QliB0"
      },
      "source": [
        "## Градиентные ленты\n",
        "\n",
        "TensorFlow предоставляет API [tf.GradientTape](https://www.tensorflow.org/api_docs/python/tf/GradientTape) для автоматической дифференциации; то есть вычисление градиента вычисления относительно некоторых входных данных, обычно `tf.Variable` s. TensorFlow «записывает» соответствующие операции, выполненные в контексте `tf.GradientTape` на «ленту». Затем TensorFlow использует эту ленту для вычисления градиентов «записанных» вычислений с использованием [дифференцирования в обратном режиме](https://en.wikipedia.org/wiki/Automatic_differentiation) .\n",
        "\n",
        "Вот простой пример:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "Xq9GgTCP7a4A"
      },
      "outputs": [

      ],
      "source": [
        "x = tf.Variable(3.0)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  y = x**2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CR9tFAP_7cra"
      },
      "source": [
        "После того, как вы записали некоторые операции, используйте `GradientTape.gradient(target, sources)` чтобы вычислить градиент некоторой цели (часто потери) относительно некоторого источника (часто переменных модели)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "LsvrwF6bHroC"
      },
      "outputs": [

      ],
      "source": [
        "# dy = 2x * dx\n",
        "dy_dx = tape.gradient(y, x)\n",
        "dy_dx.numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Q2_aqsO25Vx1"
      },
      "source": [
        "В приведенном выше примере используются скаляры, но `tf.GradientTape` работает так же легко на любом тензоре:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "vacZ3-Ws5VdV"
      },
      "outputs": [

      ],
      "source": [
        "w = tf.Variable(tf.random.normal((3, 2)), name='w')\n",
        "b = tf.Variable(tf.zeros(2, dtype=tf.float32), name='b')\n",
        "x = [[1., 2., 3.]]\n",
        "\n",
        "with tf.GradientTape(persistent=True) as tape:\n",
        "  y = x @ w + b\n",
        "  loss = tf.reduce_mean(y**2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "i4eXOkrQ-9Pb"
      },
      "source": [
        "Чтобы получить градиент `y` относительно обеих переменных, вы можете передать оба источника в метод `gradient` . Лента обладает гибкостью в отношении передачи источников и будет принимать любую вложенную комбинацию списков или словарей и возвращать градиент, структурированный таким же образом (см. `tf.nest` )."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "luOtK1Da_BR0"
      },
      "outputs": [

      ],
      "source": [
        "[dl_dw, dl_db] = tape.gradient(loss, [w, b])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ei4iVXi6qgM7"
      },
      "source": [
        "Градиент по отношению к каждому источнику имеет форму источника:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "aYbWRFPZqk4U"
      },
      "outputs": [

      ],
      "source": [
        "print(w.shape)\n",
        "print(dl_dw.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dI_SzxHsvao1"
      },
      "source": [
        "Вот снова вычисление градиента, на этот раз передавая словарь переменных:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "d73cY6NOuaMd"
      },
      "outputs": [

      ],
      "source": [
        "my_vars = {\n",
        "    'w': tf.Variable(tf.random.normal((3, 2)), name='w'),\n",
        "    'b': tf.Variable(tf.zeros(2, dtype=tf.float32), name='b')\n",
        "}\n",
        "\n",
        "grad = tape.gradient(loss, my_vars)\n",
        "grad['b']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HZ2LvHifEMgO"
      },
      "source": [
        "## Градиенты по отношению к модели\n",
        "\n",
        "Распространено собирать `tf.Variables` в `tf.Module` или в один из его подклассов ( `layers.Layer` , `keras.Model` ) для [проверки](checkpoint.ipynb) и [экспорта](saved_model.ipynb) .\n",
        "\n",
        "В большинстве случаев вы захотите рассчитать градиенты относительно обучаемых переменных модели. Поскольку все подклассы `tf.Module` агрегируют свои переменные в свойстве `Module.trainable_variables` , вы можете рассчитать эти градиенты в нескольких строках кода: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "JvesHtbQESc-"
      },
      "outputs": [

      ],
      "source": [
        "layer = tf.keras.layers.Dense(2, activation='relu')\n",
        "x = tf.constant([[1., 2., 3.]])\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  # Forward pass\n",
        "  y = layer(x)\n",
        "  loss = tf.reduce_mean(y**2)\n",
        "\n",
        "# Calculate gradients with respect to every trainable variable\n",
        "grad = tape.gradient(loss, layer.trainable_variables)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "PR_ezr6UFrpI"
      },
      "outputs": [

      ],
      "source": [
        "for var, g in zip(layer.trainable_variables, grad):\n",
        "  print(f'{var.name}, shape: {g.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "f6Gx6LS714zR"
      },
      "source": [
        "<a id=\"watches\"></a>\n",
        "\n",
        "## Управление тем, что смотрит лента"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "N4VlqKFzzGaC"
      },
      "source": [
        "Поведение по умолчанию - записывать все операции после доступа к обучаемому `tf.Variable` . Причины этого:\n",
        "\n",
        "- Лента должна знать, какие операции записывать в прямом проходе, чтобы рассчитать градиенты в обратном проходе.\n",
        "- Лента содержит ссылки на промежуточные выходы, поэтому вы не хотите записывать ненужные операции.\n",
        "- Наиболее распространенный вариант использования включает вычисление градиента потерь по всем обучаемым переменным модели.\n",
        "\n",
        "Например, следующее не может рассчитать градиент, потому что `tf.Tensor` не «отслеживается» по умолчанию, а `tf.Variable` не обучаем:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "Kj9gPckdB37a"
      },
      "outputs": [

      ],
      "source": [
        "# A trainable variable\n",
        "x0 = tf.Variable(3.0, name='x0')\n",
        "# Not trainable\n",
        "x1 = tf.Variable(3.0, name='x1', trainable=False)\n",
        "# Not a Variable: A variable + tensor returns a tensor.\n",
        "x2 = tf.Variable(2.0, name='x2') + 1.0\n",
        "# Not a variable\n",
        "x3 = tf.constant(3.0, name='x3')\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  y = (x0**2) + (x1**2) + (x2**2)\n",
        "\n",
        "grad = tape.gradient(y, [x0, x1, x2, x3])\n",
        "\n",
        "for g in grad:\n",
        "  print(g)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RkcpQnLgNxgi"
      },
      "source": [
        "Вы можете перечислить переменные, отслеживаемые лентой, используя метод `GradientTape.watched_variables` :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "hwNwjW1eAkib"
      },
      "outputs": [

      ],
      "source": [
        "[var.name for var in tape.watched_variables()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NB9I1uFvB4tf"
      },
      "source": [
        "`tf.GradientTape` предоставляет хуки, которые дают пользователю контроль над тем, что просматривается или не отслеживается.\n",
        "\n",
        "Чтобы записать градиенты относительно `tf.Tensor` , вам нужно вызвать `GradientTape.watch(x)` :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "tVN1QqFRDHBK"
      },
      "outputs": [

      ],
      "source": [
        "x = tf.constant(3.0)\n",
        "with tf.GradientTape() as tape:\n",
        "  tape.watch(x)\n",
        "  y = x**2\n",
        "\n",
        "# dy = 2x * dx\n",
        "dy_dx = tape.gradient(y, x)\n",
        "print(dy_dx.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qxsiYnf2DN8K"
      },
      "source": [
        "И наоборот, чтобы отключить поведение по умолчанию просмотра всех `tf.Variables` , установите `watch_accessed_variables=False` при создании ленты градиента. Этот расчет использует две переменные, но связывает градиент только для одной из переменных:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "7QPzwWvSEwIp"
      },
      "outputs": [

      ],
      "source": [
        "x0 = tf.Variable(0.0)\n",
        "x1 = tf.Variable(10.0)\n",
        "\n",
        "with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
        "  tape.watch(x1)\n",
        "  y0 = tf.math.sin(x0)\n",
        "  y1 = tf.nn.softplus(x1)\n",
        "  y = y0 + y1\n",
        "  ys = tf.reduce_sum(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TRduLbE1H2IJ"
      },
      "source": [
        "Поскольку `GradientTape.watch` не был вызван в `x0` , градиент не вычисляется относительно него:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "e6GM-3evH1Sz"
      },
      "outputs": [

      ],
      "source": [
        "# dy = 2x * dx\n",
        "grad = tape.gradient(ys, {'x0': x0, 'x1': x1})\n",
        "\n",
        "print('dy/dx0:', grad['x0'])\n",
        "print('dy/dx1:', grad['x1'].numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2g1nKB6P-OnA"
      },
      "source": [
        "## Промежуточные результаты\n",
        "\n",
        "Вы также можете запросить градиенты вывода относительно промежуточных значений, вычисленных в контексте `tf.GradientTape` ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "7XaPRAwUyYms"
      },
      "outputs": [

      ],
      "source": [
        "x = tf.constant(3.0)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  tape.watch(x)\n",
        "  y = x * x\n",
        "  z = y * y\n",
        "\n",
        "# Use the tape to compute the gradient of z with respect to the\n",
        "# intermediate value y.\n",
        "# dz_dx = 2 * y, where y = x ** 2\n",
        "print(tape.gradient(z, y).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ISkXuY7YzIcS"
      },
      "source": [
        "By default, the resources held by a `GradientTape` are released as soon as `GradientTape.gradient()` method is called. To compute multiple gradients over the same computation, create a `persistent` gradient tape. This allows multiple calls to the `gradient()` method as resources are released when the tape object is garbage collected. For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "zZaCm3-9zVCi"
      },
      "outputs": [

      ],
      "source": [
        "x = tf.constant([1, 3.0])\n",
        "with tf.GradientTape(persistent=True) as tape:\n",
        "  tape.watch(x)\n",
        "  y = x * x\n",
        "  z = y * y\n",
        "\n",
        "print(tape.gradient(z, x).numpy())  # 108.0 (4 * x**3 at x = 3)\n",
        "print(tape.gradient(y, x).numpy())  # 6.0 (2 * x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "j8bv_jQFg6CN"
      },
      "outputs": [

      ],
      "source": [
        "del tape   # Drop the reference to the tape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "O_ZY-9BUB7vX"
      },
      "source": [
        "## Примечания по производительности\n",
        "\n",
        "- Есть небольшие накладные расходы, связанные с выполнением операций в контексте градиентной ленты. Для большинства стремящихся к исполнению это не будет заметной стоимостью, но вы все равно должны использовать контекст ленты только в тех областях, где это требуется.\n",
        "\n",
        "- Градиентные ленты используют память для хранения промежуточных результатов, включая входы и выходы, для использования во время обратного прохода.\n",
        "\n",
        "    Для эффективности некоторым операциям (например, `ReLU` ) не нужно сохранять промежуточные результаты, и они сокращаются во время прямого прохода. Однако, если вы используете `persistent=True` на вашей ленте, *ничего не удаляется,* и ваше пиковое использование памяти будет выше."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9dLBpZsJebFq"
      },
      "source": [
        "## Градиенты нескалярных целей"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7pldU9F5duP2"
      },
      "source": [
        "Градиент - это в основном операция на скаляре."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "qI0sDV_WeXBb"
      },
      "outputs": [

      ],
      "source": [
        "x = tf.Variable(2.0)\n",
        "with tf.GradientTape(persistent=True) as tape:\n",
        "  y0 = x**2\n",
        "  y1 = 1 / x\n",
        "\n",
        "print(tape.gradient(y0, x).numpy())\n",
        "print(tape.gradient(y1, x).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "COEyYp34fxj4"
      },
      "source": [
        "Таким образом, если вы запрашиваете градиент нескольких целей, результат для каждого источника:\n",
        "\n",
        "- Градиент суммы целей или эквивалентно\n",
        "- Сумма градиентов каждой цели."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "o4a6_YOcfWKS"
      },
      "outputs": [

      ],
      "source": [
        "x = tf.Variable(2.0)\n",
        "with tf.GradientTape() as tape:\n",
        "  y0 = x**2\n",
        "  y1 = 1 / x\n",
        "\n",
        "print(tape.gradient({'y0': y0, 'y1': y1}, x).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uvP-mkBMgbym"
      },
      "source": [
        "Аналогично, если цель (и) не являются скалярными, вычисляется градиент суммы:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "DArPWqsSh5un"
      },
      "outputs": [

      ],
      "source": [
        "x = tf.Variable(2.)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  y = x * [3., 4.]\n",
        "\n",
        "print(tape.gradient(y, x).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "flDbx68Zh5Lb"
      },
      "source": [
        "Это упрощает выбор градиента суммы совокупных потерь или градиента суммы поэлементного расчета убытков.\n",
        "\n",
        "Если вам нужен отдельный градиент для каждого предмета, смотрите [Якобиан](advanced_autodiff.ipynb#jacobians) ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iwFswok8RAly"
      },
      "source": [
        "В некоторых случаях вы можете пропустить якобиана. Для поэлементного вычисления градиент суммы дает производную каждого элемента по отношению к его входному элементу, поскольку каждый элемент независим:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "JQvk_jnMmTDS"
      },
      "outputs": [

      ],
      "source": [
        "x = tf.linspace(-10.0, 10.0, 200+1)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  tape.watch(x)\n",
        "  y = tf.nn.sigmoid(x)\n",
        "\n",
        "dy_dx = tape.gradient(y, x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "e_f2QgDPmcPE"
      },
      "outputs": [

      ],
      "source": [
        "plt.plot(x, y, label='y')\n",
        "plt.plot(x, dy_dx, label='dy/dx')\n",
        "plt.legend()\n",
        "_ = plt.xlabel('x')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6kADybtQzYj4"
      },
      "source": [
        "## Контроль потока\n",
        "\n",
        "Поскольку ленты записывают операции по мере их выполнения, поток управления Python (с использованием `if` s и `while` s) обрабатывается естественным образом.\n",
        "\n",
        "Здесь разные переменные используются в каждой ветви `if` . Градиент соединяется только с той переменной, которая была использована:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "ciFLizhrrjy7"
      },
      "outputs": [

      ],
      "source": [
        "x = tf.constant(1.0)\n",
        "\n",
        "v0 = tf.Variable(2.0)\n",
        "v1 = tf.Variable(2.0)\n",
        "\n",
        "with tf.GradientTape(persistent=True) as tape:\n",
        "  tape.watch(x)\n",
        "  if x > 0.0:\n",
        "    result = v0\n",
        "  else:\n",
        "    result = v1**2 \n",
        "\n",
        "dv0, dv1 = tape.gradient(result, [v0, v1])\n",
        "\n",
        "print(dv0)\n",
        "print(dv1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HKnLaiapsjeP"
      },
      "source": [
        "Просто помните, что сами управляющие операторы не дифференцируемы, поэтому они невидимы для оптимизаторов на основе градиента.\n",
        "\n",
        "В зависимости от значения `x` в приведенном выше примере лента либо записывает `result = v0` либо `result = v1**2` . Градиент по отношению к `x` всегда равен `None` ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "8k05WmuAwPm7"
      },
      "outputs": [

      ],
      "source": [
        "dx = tape.gradient(result, x)\n",
        "\n",
        "print(dx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "egypBxISAHhx"
      },
      "source": [
        "## Получение градиента `None`\n",
        "\n",
        "Когда цель не связана с источником, вы получите градиент `None` .\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "CU185WDM81Ut"
      },
      "outputs": [

      ],
      "source": [
        "x = tf.Variable(2.)\n",
        "y = tf.Variable(3.)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  z = y * y\n",
        "print(tape.gradient(z, x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sZbKpHfBRJym"
      },
      "source": [
        "Здесь `z` , очевидно, не связан с `x` , но есть несколько менее очевидных способов отсоединения градиента."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eHDzDOiQ8xmw"
      },
      "source": [
        "### 1. Заменил переменную на тензор.\n",
        "\n",
        "В разделе [«Управление тем, что смотрит лента»,](#watches) вы увидели, что лента будет автоматически наблюдать `tf.Variable` но не `tf.Tensor` .\n",
        "\n",
        "Одна из распространенных ошибок заключается в непреднамеренной замене `tf.Variable` на `tf.Tensor` вместо использования `Variable.assign` для обновления `tf.Variable` . Вот пример:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "QPKY4Tn9zX7_"
      },
      "outputs": [

      ],
      "source": [
        "x = tf.Variable(2.0)\n",
        "\n",
        "for epoch in range(2):\n",
        "  with tf.GradientTape() as tape:\n",
        "    y = x+1\n",
        "\n",
        "  print(type(x).__name__, \":\", tape.gradient(y, x))\n",
        "  x = x + 1   # This should be `x.assign_add(1)`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3gwZKxgA97an"
      },
      "source": [
        "### 2. Делали расчеты вне TensorFlow\n",
        "\n",
        "Лента не может записать путь градиента, если расчет выходит из TensorFlow. Например:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "jmoLCDJb_yw1"
      },
      "outputs": [

      ],
      "source": [
        "x = tf.Variable([[1.0, 2.0],\n",
        "                 [3.0, 4.0]], dtype=tf.float32)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  x2 = x**2\n",
        "\n",
        "  # This step is calculated with NumPy\n",
        "  y = np.mean(x2, axis=0)\n",
        "\n",
        "  # Like most ops, reduce_mean will cast the NumPy array to a constant tensor\n",
        "  # using `tf.convert_to_tensor`.\n",
        "  y = tf.reduce_mean(y, axis=0)\n",
        "\n",
        "print(tape.gradient(y, x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "p3YVfP3R-tp7"
      },
      "source": [
        "### 3. Взял градиенты через целое число или строку\n",
        "\n",
        "Целые числа и строки не дифференцируемы. Если путь расчета использует эти типы данных, градиента не будет.\n",
        "\n",
        "Никто не ожидает, что строки будут дифференцируемыми, но легко создать случайную константу или переменную `int` если вы не укажете `dtype` ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "9jlHXHqfASU3"
      },
      "outputs": [

      ],
      "source": [
        "# The x0 variable has an `int` dtype.\n",
        "x = tf.Variable([[2, 2],\n",
        "                 [2, 2]])\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  # The path to x1 is blocked by the `int` dtype here.\n",
        "  y = tf.cast(x, tf.float32)\n",
        "  y = tf.reduce_sum(x)\n",
        "\n",
        "print(tape.gradient(y, x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RsdP_mTHX9L1"
      },
      "source": [
        "TensorFlow не выполняет автоматическое приведение между типами, поэтому на практике вы часто получаете ошибку типа вместо отсутствующего градиента."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WyAZ7C8qCEs6"
      },
      "source": [
        "### 5. Взял градиенты через объект с состоянием\n",
        "\n",
        "Состояние останавливает градиенты. Когда вы читаете из объекта с состоянием, лента может видеть только текущее состояние, а не историю, которая к нему ведет.\n",
        "\n",
        "`tf.Tensor` является неизменным. Вы не можете изменить тензор, как только он создан. У него есть *значение* , но нет *государства* . Все обсуждаемые до сих пор операции также не имеют состояния: вывод `tf.matmul` зависит только от его входных данных.\n",
        "\n",
        "`tf.Variable` имеет внутреннее состояние, его значение. Когда вы используете переменную, состояние читается. Это нормально для вычисления градиента относительно переменной, но состояние переменной блокирует вычисления градиента от дальнейшего продвижения назад. Например:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "C1tLeeRFE479"
      },
      "outputs": [

      ],
      "source": [
        "x0 = tf.Variable(3.0)\n",
        "x1 = tf.Variable(0.0)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  # Update x1 = x1 + x0.\n",
        "  x1.assign_add(x0)\n",
        "  # The tape starts recording from x1.\n",
        "  y = x1**2   # y = (x1 + x0)**2\n",
        "\n",
        "# This doesn't work.\n",
        "print(tape.gradient(y, x0))   #dy/dx0 = 2*(x1 + x2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xKA92-dqF2r-"
      },
      "source": [
        "Аналогично, итераторы `tf.data.Dataset` и `tf.queue` s сохраняют состояние и останавливают все градиенты на тензорах, проходящих через них."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HHvcDGIbOj2I"
      },
      "source": [
        "## Градиент не зарегистрирован"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aoc-A6AxVqry"
      },
      "source": [
        "Some `tf.Operation`s are **registered as being non-differentiable** and will return `None`. Others have **no gradient registered**.\n",
        "\n",
        "The [tf.raw_ops](https://www.tensorflow.org/api_docs/python/tf/raw_ops) page shows which low-level ops have gradients registered.\n",
        "\n",
        "Если вы попытаетесь выполнить градиент через операцию с плавающей точкой, у которой нет зарегистрированного градиента, лента выдаст ошибку, а не вернет `None` . Таким образом, вы знаете, что что-то пошло не так.\n",
        "\n",
        "Например, функция `tf.image.adjust_contrast` оборачивает [raw_ops.AdjustContrastv2,](https://www.tensorflow.org/api_docs/python/tf/raw_ops#.AdjustContrastv2) который может иметь градиент, но градиент не реализован:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "HSb20FXc_V0U"
      },
      "outputs": [

      ],
      "source": [
        "image = tf.Variable([[[0.5, 0.0, 0.0]]])\n",
        "delta = tf.Variable(0.1)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  new_image = tf.image.adjust_contrast(image, delta)\n",
        "\n",
        "try:\n",
        "  print(tape.gradient(new_image, [image, delta]))\n",
        "  assert False   # This should not happen.\n",
        "except LookupError as e:\n",
        "  print(f'{type(e).__name__}: {e}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pDoutjzATiEm"
      },
      "source": [
        "Если вам нужно провести различие между этими операциями, вам нужно либо реализовать градиент и зарегистрировать его (используя `tf.RegisterGradient` ), либо повторно реализовать функцию, используя другие операции."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GCTwc_dQXp2W"
      },
      "source": [
        "## Нули вместо None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TYDrVogA89eA"
      },
      "source": [
        "В некоторых случаях было бы удобно получить 0 вместо `None` для неподключенных градиентов. Вы можете решить, что возвращать, когда у вас есть неподключенные градиенты, используя аргумент `unconnected_gradients` :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "U6zxk1sf9Ixx"
      },
      "outputs": [

      ],
      "source": [
        "x = tf.Variable([2., 2.])\n",
        "y = tf.Variable(3.)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  z = y**2\n",
        "print(tape.gradient(z, x, unconnected_gradients=tf.UnconnectedGradients.ZERO))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Tce3stUlHN0L"
      ],
      "name": "autodiff.ipynb",
      "private_outputs": true,
      "provenance": [

      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
